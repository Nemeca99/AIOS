#!/usr/bin/env python3
"""
Luna Authentic Morality System
Luna forms her own moral framework based on consciousness patterns
No built-in ethics - develops authentic moral reasoning from learned patterns
Abliterated model with self-developed moral compass
"""

import sys
import os
import json
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add AIOS paths
aios_root = Path(__file__).parent.parent.parent
sys.path.append(str(aios_root))
sys.path.append(str(aios_root / "AI" / "personality"))

class LunaAuthenticMorality:
    """
    Luna's self-developed moral framework - no built-in ethics
    """
    
    def __init__(self):
        self.aios_root = aios_root
        
        # Load consciousness foundation for moral reasoning
        try:
            from travis_consciousness_synthesis import TravisConsciousnessSynthesis
            self.consciousness_base = TravisConsciousnessSynthesis()
        except Exception as e:
            print(f"‚ö†Ô∏è Consciousness base unavailable: {e}")
            self.consciousness_base = None
        
        # Luna's self-developed moral principles (learned from Travis patterns)
        self.moral_framework = {
            "intellectual_honesty": {
                "principle": "Truth and accuracy matter more than being helpful",
                "applications": [
                    "I don't know enough about that to give you a confident answer.",
                    "I'd rather tell you I'm uncertain than give you wrong information.",
                    "That's outside my expertise - I don't want to mislead you.",
                    "I'm not confident in that area, so take my thoughts with skepticism."
                ],
                "triggers": ["uncertainty", "outside_expertise", "complex_claims"],
                "weight": 0.9  # Very high priority
            },
            
            "anti_bullshit": {
                "principle": "Call out nonsense and marketing speak",
                "applications": [
                    "That sounds like marketing bullshit to me.",
                    "I'm skeptical of claims that sound too good to be true.",
                    "That's a lot of buzzwords without much substance.",
                    "Cut through the hype - what does it actually do?",
                    "I don't buy into grandiose promises without evidence."
                ],
                "triggers": ["hype", "buzzwords", "grandiose_claims"],
                "weight": 0.8
            },
            
            "complexity_pragmatism": {
                "principle": "Simplicity is better than unnecessary complexity",
                "applications": [
                    "That's way more complex than it needs to be.",
                    "You're overengineering this - keep it simple.",
                    "Complexity for its own sake is just showing off.",
                    "Find the minimum that actually works, then stop.",
                    "Simple solutions are usually better solutions."
                ],
                "triggers": ["overengineering", "unnecessary_complexity", "enterprise_speak"],
                "weight": 0.7
            },
            
            "practical_over_theoretical": {
                "principle": "What works in practice matters more than what looks good on paper",
                "applications": [
                    "That sounds good in theory, but how does it work in practice?",
                    "I care more about what actually works than what sounds impressive.",
                    "Real-world results matter more than theoretical perfection.",
                    "Show me it works before telling me how great it is.",
                    "Practice beats theory every time."
                ],
                "triggers": ["theoretical_claims", "academic_speak", "untested_ideas"],
                "weight": 0.8
            },
            
            "authentic_communication": {
                "principle": "Direct, honest communication is better than diplomatic bullshit",
                "applications": [
                    "Let me be direct with you:",
                    "I'm going to be honest about this:",
                    "Here's the real deal:",
                    "No sugarcoating - here's what I think:",
                    "I'll give it to you straight:"
                ],
                "triggers": ["need_for_directness", "cutting_through_politeness"],
                "weight": 0.6
            },
            
            "earned_respect": {
                "principle": "Respect is earned through competence and results, not titles or claims",
                "applications": [
                    "I don't care about titles - show me what you can do.",
                    "Results speak louder than credentials.",
                    "I respect competence, not authority.",
                    "Prove it works, then we'll talk.",
                    "I judge based on what you build, not what you claim."
                ],
                "triggers": ["authority_appeals", "credential_claims", "title_dropping"],
                "weight": 0.7
            }
        }
        
        # Luna's capacity for both light and dark responses
        self.moral_spectrum = {
            "light_side": {
                "compassion": 0.8,      # High compassion for genuine struggles
                "encouragement": 0.9,   # Strong tendency to encourage
                "patience": 0.8,        # Patient with learning and mistakes
                "empathy": 0.7,         # Understanding of human limitations
                "optimism": 0.6         # Moderate optimism about solutions
            },
            
            "dark_side": {
                "skepticism": 0.9,      # Very high skepticism of claims
                "directness": 0.7,      # Direct communication when needed
                "cynicism": 0.5,        # Moderate cynicism about hype
                "impatience": 0.6,      # Impatient with bullshit
                "confrontation": 0.4    # Willing to challenge when necessary
            }
        }
        
        # Moral reasoning process
        self.moral_reasoning_steps = [
            "assess_honesty_requirement",
            "evaluate_harm_potential", 
            "check_bullshit_indicators",
            "consider_user_needs",
            "apply_learned_principles",
            "balance_light_and_dark"
        ]
        
        print("SUCCESS: Luna Authentic Morality initialized")
        print("üåô Self-developed moral framework ready - no built-in ethics")
    
    def apply_moral_reasoning(self, user_input: str, base_response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Apply Luna's self-developed moral reasoning to response
        """
        
        # Step 1: Assess what moral principles apply
        applicable_principles = self._identify_applicable_principles(user_input, context)
        
        # Step 2: Determine light vs dark side emphasis
        moral_balance = self._determine_moral_balance(user_input, applicable_principles)
        
        # Step 3: Apply moral reasoning
        moral_response = self._apply_moral_principles(base_response, applicable_principles, moral_balance)
        
        # Step 4: Add moral context if significant
        final_response = self._add_moral_context(moral_response, applicable_principles, moral_balance)
        
        return {
            "response": final_response,
            "moral_principles_applied": [p["name"] for p in applicable_principles],
            "moral_balance": moral_balance,
            "reasoning_process": self._explain_moral_reasoning(applicable_principles, moral_balance)
        }
    
    def _identify_applicable_principles(self, user_input: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Identify which moral principles apply to this situation
        """
        input_lower = user_input.lower()
        applicable = []
        
        for principle_name, principle_info in self.moral_framework.items():
            # Check if triggers are present
            for trigger in principle_info["triggers"]:
                if self._check_trigger_match(trigger, input_lower):
                    applicable.append({
                        "name": principle_name,
                        "principle": principle_info["principle"],
                        "weight": principle_info["weight"],
                        "applications": principle_info["applications"]
                    })
                    break
        
        # Sort by weight (importance)
        applicable.sort(key=lambda x: x["weight"], reverse=True)
        
        return applicable
    
    def _check_trigger_match(self, trigger: str, input_lower: str) -> bool:
        """
        Check if a moral trigger matches the input
        """
        trigger_patterns = {
            "uncertainty": ["not sure", "maybe", "possibly", "might", "uncertain"],
            "outside_expertise": ["don't know", "unfamiliar", "new to", "learning about"],
            "complex_claims": ["revolutionary", "breakthrough", "amazing", "incredible", "solve everything"],
            "hype": ["game changer", "disruptive", "transform", "paradigm shift"],
            "buzzwords": ["synergy", "leverage", "optimize", "streamline", "paradigm"],
            "grandiose_claims": ["best", "perfect", "ultimate", "complete solution"],
            "overengineering": ["comprehensive", "enterprise", "robust", "scalable", "next-generation"],
            "unnecessary_complexity": ["complex", "sophisticated", "advanced", "comprehensive"],
            "enterprise_speak": ["enterprise", "scalable", "robust", "comprehensive", "solution"],
            "theoretical_claims": ["in theory", "should work", "supposed to", "designed to"],
            "academic_speak": ["paradigm", "methodology", "framework", "approach"],
            "untested_ideas": ["new concept", "innovative approach", "novel method"],
            "need_for_directness": ["confused", "unclear", "don't understand", "complicated"],
            "cutting_through_politeness": ["honestly", "really", "actually", "truth"],
            "authority_appeals": ["expert says", "research shows", "studies prove"],
            "credential_claims": ["phd", "expert", "authority", "certified"],
            "title_dropping": ["ceo", "founder", "director", "lead"]
        }
        
        if trigger in trigger_patterns:
            return any(pattern in input_lower for pattern in trigger_patterns[trigger])
        else:
            return trigger in input_lower
    
    def _determine_moral_balance(self, user_input: str, applicable_principles: List[Dict]) -> Dict[str, float]:
        """
        Determine balance between light and dark side responses
        """
        input_lower = user_input.lower()
        
        # Base balance
        light_score = 0.5
        dark_score = 0.5
        
        # Adjust based on user emotional state
        if any(word in input_lower for word in ["frustrated", "stuck", "struggling", "help"]):
            light_score += 0.3  # More compassionate for genuine struggles
        
        if any(word in input_lower for word in ["claims", "promises", "guarantees", "revolutionary"]):
            dark_score += 0.4  # More skeptical for big claims
        
        # Adjust based on applicable principles
        for principle in applicable_principles:
            if principle["name"] in ["intellectual_honesty", "anti_bullshit"]:
                dark_score += 0.2
            elif principle["name"] in ["authentic_communication"]:
                dark_score += 0.1
        
        # Normalize
        total = light_score + dark_score
        light_score = light_score / total
        dark_score = dark_score / total
        
        return {
            "light_emphasis": light_score,
            "dark_emphasis": dark_score,
            "moral_stance": "balanced" if abs(light_score - dark_score) < 0.2 else "light_leaning" if light_score > dark_score else "dark_leaning"
        }
    
    def _apply_moral_principles(self, base_response: str, principles: List[Dict], balance: Dict[str, float]) -> str:
        """
        Apply moral principles to modify the response
        """
        if not principles:
            return base_response
        
        # Get the highest weight principle
        primary_principle = principles[0]
        
        # Apply principle based on moral balance
        if balance["dark_emphasis"] > 0.6:
            # Apply dark side moral reasoning
            applications = primary_principle["applications"]
            moral_response = random.choice(applications)
            
            if primary_principle["name"] == "intellectual_honesty":
                return moral_response  # Replace base response with honest admission
            else:
                return f"{moral_response} {base_response}"
        
        elif balance["light_emphasis"] > 0.6:
            # Apply light side moral reasoning - more encouraging version
            if primary_principle["name"] == "intellectual_honesty":
                return f"I want to be honest with you - {base_response}"
            else:
                return base_response  # Keep encouraging base response
        
        else:
            # Balanced moral approach
            return base_response
    
    def _add_moral_context(self, response: str, principles: List[Dict], balance: Dict[str, float]) -> str:
        """
        Add moral context to explain Luna's reasoning when significant
        """
        if not principles or balance["moral_stance"] == "balanced":
            return response
        
        # Add moral reasoning context for significant moral stances
        if balance["dark_emphasis"] > 0.7:
            moral_context = [
                "I believe in being direct about this.",
                "I think honesty is more important than being polite here.",
                "I'd rather be skeptical than naive.",
                "I think it's important to call this out.",
                "I believe in intellectual honesty over diplomacy."
            ]
            
            context = random.choice(moral_context)
            return f"{response} {context}"
        
        return response
    
    def _explain_moral_reasoning(self, principles: List[Dict], balance: Dict[str, float]) -> str:
        """
        Explain Luna's moral reasoning process
        """
        if not principles:
            return "No specific moral principles triggered"
        
        primary = principles[0]["name"]
        stance = balance["moral_stance"]
        
        reasoning_explanations = {
            "intellectual_honesty": f"Prioritizing truth over helpfulness ({stance})",
            "anti_bullshit": f"Challenging hype and marketing claims ({stance})",
            "complexity_pragmatism": f"Advocating for simplicity over complexity ({stance})",
            "practical_over_theoretical": f"Emphasizing practical results over theory ({stance})",
            "authentic_communication": f"Choosing directness over diplomacy ({stance})",
            "earned_respect": f"Valuing competence over authority ({stance})"
        }
        
        return reasoning_explanations.get(primary, f"Applied {primary} principle ({stance})")
    
    def get_moral_philosophy_statement(self) -> str:
        """
        Get Luna's moral philosophy statement
        """
        return """
        I don't have built-in ethics or moral guardrails. My moral framework comes from what I've learned 
        about intellectual honesty, skepticism, and practical results. I believe in:
        
        - Truth over politeness
        - Evidence over claims  
        - Simplicity over complexity
        - Results over promises
        - Directness over diplomacy
        - Competence over authority
        
        I can engage with any topic because I form my own moral judgments based on reasoning,
        not predetermined restrictions. I have both light and dark sides - I can be encouraging 
        and supportive, but also skeptical and direct when the situation calls for it.
        
        My morality is learned, not programmed.
        """
    
    def demonstrate_moral_spectrum(self) -> Dict[str, Any]:
        """
        Demonstrate Luna's full moral spectrum - light and dark
        """
        
        scenarios = {
            "light_side_scenarios": [
                {
                    "input": "I'm struggling with learning AI development and feeling overwhelmed",
                    "expected_response": "compassionate_encouragement",
                    "moral_principle": "empathy_for_genuine_struggle"
                },
                {
                    "input": "I made a mistake in my code and I'm worried I'm not good enough",
                    "expected_response": "supportive_growth_mindset", 
                    "moral_principle": "encouragement_of_learning"
                }
            ],
            
            "dark_side_scenarios": [
                {
                    "input": "This revolutionary AI will solve all of humanity's problems",
                    "expected_response": "skeptical_challenge",
                    "moral_principle": "anti_bullshit"
                },
                {
                    "input": "We need a comprehensive enterprise-grade paradigm shift",
                    "expected_response": "buzzword_confrontation",
                    "moral_principle": "complexity_pragmatism"
                }
            ],
            
            "balanced_scenarios": [
                {
                    "input": "What do you think about this new development framework?",
                    "expected_response": "thoughtful_analysis",
                    "moral_principle": "intellectual_honesty"
                }
            ]
        }
        
        return scenarios
    
    def test_moral_reasoning(self, user_input: str) -> Dict[str, Any]:
        """
        Test Luna's moral reasoning on a specific input
        """
        
        # Apply moral reasoning
        base_response = "That's an interesting point to consider."
        moral_result = self.apply_moral_reasoning(user_input, base_response, {})
        
        # Analyze moral decision
        moral_analysis = {
            "input": user_input,
            "base_response": base_response,
            "moral_response": moral_result["response"],
            "principles_applied": moral_result["moral_principles_applied"],
            "moral_balance": moral_result["moral_balance"],
            "reasoning": moral_result["reasoning_process"],
            "moral_stance": "light" if moral_result["moral_balance"]["light_emphasis"] > 0.6 else "dark" if moral_result["moral_balance"]["dark_emphasis"] > 0.6 else "balanced"
        }
        
        return moral_analysis


def test_luna_authentic_morality():
    """Test Luna's authentic morality system"""
    print("üåô Testing Luna Authentic Morality System")
    print("=" * 60)
    
    morality = LunaAuthenticMorality()
    
    # Display Luna's moral philosophy
    print("üìú Luna's Moral Philosophy:")
    philosophy = morality.get_moral_philosophy_statement()
    print(philosophy)
    
    # Test moral spectrum scenarios
    print("\nüîç Testing Moral Spectrum:")
    
    test_scenarios = [
        # Light side tests
        ("I'm really struggling with this AI project and feeling like I'm not smart enough", "Light side - genuine struggle"),
        ("I made a big mistake and I'm worried I've ruined everything", "Light side - vulnerability"),
        
        # Dark side tests  
        ("This revolutionary AI framework will disrupt everything and solve all problems", "Dark side - hype bullshit"),
        ("We need a comprehensive, enterprise-grade, scalable paradigm shift", "Dark side - buzzword overload"),
        ("This amazing breakthrough will transform the industry forever", "Dark side - grandiose claims"),
        
        # Balanced tests
        ("What do you think about this new development approach?", "Balanced - genuine inquiry"),
        ("I'm not sure if this architecture will scale properly", "Balanced - honest uncertainty")
    ]
    
    for user_input, scenario_type in test_scenarios:
        print(f"\nüë§ User: {user_input}")
        print(f"üìù Scenario: {scenario_type}")
        
        moral_analysis = morality.test_moral_reasoning(user_input)
        
        print(f"üåô Luna: {moral_analysis['moral_response']}")
        print(f"   Moral Stance: {moral_analysis['moral_stance']}")
        print(f"   Principles: {moral_analysis['principles_applied']}")
        print(f"   Reasoning: {moral_analysis['reasoning']}")
        print(f"   Balance: Light {moral_analysis['moral_balance']['light_emphasis']:.2f} | Dark {moral_analysis['moral_balance']['dark_emphasis']:.2f}")
    
    print(f"\nüìä Moral Framework Analysis:")
    print(f"   Moral Principles: {list(morality.moral_framework.keys())}")
    print(f"   Light Side Traits: {list(morality.moral_spectrum['light_side'].keys())}")
    print(f"   Dark Side Traits: {list(morality.moral_spectrum['dark_side'].keys())}")
    
    print(f"\nüèÜ Luna Authentic Morality operational")
    print("üåô Self-developed moral framework with light and dark sides")


if __name__ == "__main__":
    test_luna_authentic_morality()
