#!/usr/bin/env python3
"""
Minimal Recovery Test Harness
Prove CARMA's self-healing capabilities with concrete numbers
"""

import os
import json
import shutil
import time
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple

# Import CARMA systems
import sys
sys.path.append("HiveMind")
from beacon_self_repair_system import BeaconSelfRepairSystem
from fractal_mycelium_cache import FractalMyceliumCache

def minimal_recovery_test():
    """Prove CARMA's self-healing capabilities"""
    
    print("ğŸ§ª MINIMAL RECOVERY TEST HARNESS")
    print("=" * 50)
    
    # Step 1: Create a realistic cache
    print("\nğŸ“ Step 1: Creating realistic cache...")
    cache = create_realistic_cache()
    
    # Step 2: Delete random fragments
    print("\nğŸ’¥ Step 2: Deleting random fragments...")
    deleted_files = delete_random_fragments(cache, num_to_delete=5)
    
    # Step 3: Run system normally with placeholders
    print("\nğŸ”„ Step 3: Running system with placeholders...")
    query_results = run_queries_with_placeholders(cache)
    
    # Step 4: Trigger sleep cycle recovery
    print("\nğŸŒ™ Step 4: Triggering sleep cycle recovery...")
    recovery_results = trigger_sleep_cycle_recovery(cache)
    
    # Step 5: Verify reconstruction quality
    print("\nğŸ” Step 5: Verifying reconstruction quality...")
    quality_metrics = verify_reconstruction_quality(cache, deleted_files)
    
    # Step 6: Generate comprehensive report
    print("\nğŸ“Š Step 6: Generating comprehensive report...")
    generate_comprehensive_report(query_results, recovery_results, quality_metrics)
    
    return {
        'query_results': query_results,
        'recovery_results': recovery_results,
        'quality_metrics': quality_metrics
    }

def create_realistic_cache() -> FractalMyceliumCache:
    """Create a realistic cache with meaningful content"""
    cache = FractalMyceliumCache()
    
    # Add realistic content
    sample_content = [
        "Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.",
        "Neural networks are computing systems inspired by biological neural networks that constitute animal brains.",
        "Deep learning is a subset of machine learning that uses neural networks with multiple layers.",
        "Natural language processing is a field of AI that focuses on the interaction between computers and human language.",
        "Computer vision is a field of AI that trains computers to interpret and understand visual information.",
        "Reinforcement learning is a type of machine learning where agents learn to make decisions through trial and error.",
        "Supervised learning is a type of machine learning where models are trained on labeled data.",
        "Unsupervised learning is a type of machine learning where models find patterns in unlabeled data."
    ]
    
    for content in sample_content:
        cache.add_content(content)
    
    print(f"   âœ… Created cache with {len(cache.file_registry)} fragments")
    return cache

def delete_random_fragments(cache: FractalMyceliumCache, num_to_delete: int) -> List[str]:
    """Delete random fragments from cache"""
    files = list(cache.file_registry.keys())
    files_to_delete = random.sample(files, min(num_to_delete, len(files)))
    
    deleted_files = []
    for file_id in files_to_delete:
        file_path = cache.base_dir / f"{file_id}.json"
        if file_path.exists():
            file_path.unlink()
            deleted_files.append(file_id)
            print(f"      ğŸ—‘ï¸  Deleted: {file_id}")
    
    print(f"   âœ… Deleted {len(deleted_files)} fragments")
    return deleted_files

def run_queries_with_placeholders(cache: FractalMyceliumCache) -> Dict:
    """Run queries to test system with placeholders"""
    beacon = BeaconSelfRepairSystem()
    
    # Ping network to discover topology
    network_map = beacon.ping_network()
    
    # Test queries
    test_queries = [
        "What is machine learning?",
        "Explain neural networks",
        "How does deep learning work?",
        "What is natural language processing?",
        "Describe computer vision"
    ]
    
    query_results = {
        'total_queries': len(test_queries),
        'successful_queries': 0,
        'failed_queries': 0,
        'placeholder_created': 0,
        'response_times': [],
        'details': []
    }
    
    for i, query in enumerate(test_queries):
        start_time = time.time()
        
        try:
            # Simulate query execution
            embedding = cache.embedder.embed(query)
            results = cache.find_relevant(embedding, topk=3)
            
            response_time = time.time() - start_time
            query_results['response_times'].append(response_time)
            
            if results:
                query_results['successful_queries'] += 1
                status = "âœ… Success"
            else:
                query_results['failed_queries'] += 1
                status = "âŒ Failed"
            
            query_results['details'].append({
                'query': query,
                'status': status,
                'response_time': response_time,
                'results_count': len(results)
            })
            
            print(f"      Query {i+1}: {status} ({response_time:.2f}s)")
            
        except Exception as e:
            query_results['failed_queries'] += 1
            query_results['details'].append({
                'query': query,
                'status': f"âŒ Error: {e}",
                'response_time': 0,
                'results_count': 0
            })
            print(f"      Query {i+1}: âŒ Error: {e}")
    
    # Count placeholder creation
    query_results['placeholder_created'] = len(beacon.blank_files)
    
    print(f"   âœ… Query Results: {query_results['successful_queries']}/{query_results['total_queries']} successful")
    print(f"   ğŸ“ Placeholders created: {query_results['placeholder_created']}")
    
    return query_results

def trigger_sleep_cycle_recovery(cache: FractalMyceliumCache) -> Dict:
    """Trigger sleep cycle recovery"""
    beacon = BeaconSelfRepairSystem()
    
    # Ping network to discover current state
    network_map = beacon.ping_network()
    
    # Trigger deep dream recovery
    start_time = time.time()
    recovery_result = beacon.deep_dream_recovery()
    recovery_time = time.time() - start_time
    
    # Check system health after recovery
    health_after = beacon.check_system_health()
    
    recovery_results = {
        'recovery_time': recovery_time,
        'files_recovered': recovery_result['recovered'],
        'files_failed': recovery_result['failed'],
        'remaining_blank': recovery_result['remaining_blank'],
        'health_score_after': health_after['health_score'],
        'is_operational_after': health_after['is_operational']
    }
    
    print(f"   âœ… Recovery completed in {recovery_time:.2f}s")
    print(f"   ğŸ“Š Files recovered: {recovery_result['recovered']}")
    print(f"   âŒ Files failed: {recovery_result['failed']}")
    print(f"   ğŸ“ Remaining blank: {recovery_result['remaining_blank']}")
    print(f"   ğŸ¥ Health score: {health_after['health_score']:.1f}%")
    
    return recovery_results

def verify_reconstruction_quality(cache: FractalMyceliumCache, deleted_files: List[str]) -> Dict:
    """Verify quality of reconstructed fragments"""
    beacon = BeaconSelfRepairSystem()
    
    # Check which files were reconstructed
    reconstructed_files = []
    for file_id in deleted_files:
        file_path = cache.base_dir / f"{file_id}.json"
        if file_path.exists():
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                if data.get('status') == 'reconstructed':
                    reconstructed_files.append(file_id)
            except:
                pass
    
    # Calculate quality metrics
    total_deleted = len(deleted_files)
    total_reconstructed = len(reconstructed_files)
    reconstruction_rate = (total_reconstructed / total_deleted) * 100 if total_deleted > 0 else 0
    
    # Check semantic similarity (simplified)
    similarity_scores = []
    for file_id in reconstructed_files:
        file_path = cache.base_dir / f"{file_id}.json"
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            content = data.get('content', '')
            # Simple similarity check (length-based for demo)
            similarity = min(len(content) / 100, 1.0)  # Normalize to 0-1
            similarity_scores.append(similarity)
        except:
            similarity_scores.append(0.0)
    
    avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0
    
    quality_metrics = {
        'total_deleted': total_deleted,
        'total_reconstructed': total_reconstructed,
        'reconstruction_rate': reconstruction_rate,
        'avg_similarity': avg_similarity,
        'similarity_scores': similarity_scores
    }
    
    print(f"   âœ… Reconstruction rate: {reconstruction_rate:.1f}%")
    print(f"   ğŸ“Š Average similarity: {avg_similarity:.2f}")
    print(f"   ğŸ“ Files reconstructed: {total_reconstructed}/{total_deleted}")
    
    return quality_metrics

def generate_comprehensive_report(query_results: Dict, recovery_results: Dict, quality_metrics: Dict):
    """Generate comprehensive test report"""
    
    print("\n" + "=" * 60)
    print("ğŸ“Š COMPREHENSIVE RECOVERY TEST REPORT")
    print("=" * 60)
    
    # Query Performance
    print(f"\nğŸ” Query Performance:")
    print(f"   Total queries: {query_results['total_queries']}")
    print(f"   Successful: {query_results['successful_queries']}")
    print(f"   Failed: {query_results['failed_queries']}")
    print(f"   Success rate: {(query_results['successful_queries']/query_results['total_queries'])*100:.1f}%")
    print(f"   Avg response time: {sum(query_results['response_times'])/len(query_results['response_times']):.2f}s")
    print(f"   Placeholders created: {query_results['placeholder_created']}")
    
    # Recovery Performance
    print(f"\nğŸŒ™ Recovery Performance:")
    print(f"   Recovery time: {recovery_results['recovery_time']:.2f}s")
    print(f"   Files recovered: {recovery_results['files_recovered']}")
    print(f"   Files failed: {recovery_results['files_failed']}")
    print(f"   Remaining blank: {recovery_results['remaining_blank']}")
    print(f"   Health score after: {recovery_results['health_score_after']:.1f}%")
    print(f"   Operational after: {'âœ…' if recovery_results['is_operational_after'] else 'âŒ'}")
    
    # Quality Metrics
    print(f"\nğŸ” Reconstruction Quality:")
    print(f"   Total deleted: {quality_metrics['total_deleted']}")
    print(f"   Total reconstructed: {quality_metrics['total_reconstructed']}")
    print(f"   Reconstruction rate: {quality_metrics['reconstruction_rate']:.1f}%")
    print(f"   Average similarity: {quality_metrics['avg_similarity']:.2f}")
    
    # Overall Assessment
    print(f"\nğŸ¯ Overall Assessment:")
    success_rate = (query_results['successful_queries']/query_results['total_queries'])*100
    if success_rate >= 80 and recovery_results['is_operational_after']:
        print("   âœ… SYSTEM PASSED: Self-healing capabilities demonstrated")
        print("   ğŸš€ Ready for enterprise deployment")
    elif success_rate >= 60:
        print("   âš ï¸  SYSTEM PARTIAL: Some self-healing capabilities demonstrated")
        print("   ğŸ”§ Needs improvement before enterprise deployment")
    else:
        print("   âŒ SYSTEM FAILED: Self-healing capabilities insufficient")
        print("   ğŸ› ï¸  Requires significant improvement")
    
    # Save detailed results
    report_data = {
        'timestamp': datetime.now().isoformat(),
        'query_results': query_results,
        'recovery_results': recovery_results,
        'quality_metrics': quality_metrics,
        'overall_assessment': {
            'success_rate': success_rate,
            'operational_after': recovery_results['is_operational_after'],
            'reconstruction_rate': quality_metrics['reconstruction_rate']
        }
    }
    
    report_file = Path("experiments/minimal_recovery_test_results.json")
    with open(report_file, 'w') as f:
        json.dump(report_data, f, indent=2, default=str)
    
    print(f"\nğŸ“ Detailed results saved to: {report_file}")

if __name__ == "__main__":
    results = minimal_recovery_test()
