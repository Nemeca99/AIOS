# AIOS Clean Model Benchmark Results

## Overview
This document tracks benchmark results for different models used in the AIOS Clean system. All benchmarks use real model responses (no hardcoded responses) to ensure accurate performance measurements.

## Benchmark Methodology

### Test Setup
- **Questions**: 6 questions across complexity levels (trivial, moderate, complex)
- **Execution Mode**: Real (forced) - all responses generated by actual models
- **Metrics**: Latency, success rate, response quality
- **Environment**: LM Studio with speculative decoding enabled

### Test Questions
1. "hi" (trivial)
2. "hello" (trivial) 
3. "How are you?" (moderate)
4. "What's your name?" (moderate)
5. "Explain quantum computing in simple terms" (complex)
6. "How does machine learning work?" (complex)

## Model Comparison Results

### Model 1: Llama 3.2 PKD Deckard Almost Human (Abliterated Uncensored)
- **Model**: `llama-3.2-pkd-deckard-almost-human-abliterated-uncensored-7b-i1`
- **Draft Model**: `llama-3.2-1b-instruct-abliterated`
- **Quantization**: Main model (unknown), Draft q1_m
- **Test Date**: 2025-10-03 18:59:14

#### Performance Metrics
- **Average Latency**: 16,399ms
- **Total Time**: 98.4s
- **Success Rate**: 6/6 (100%)
- **Draft Efficiency**: 57-67% (mixed model speculative decoding)

#### Detailed Latency Breakdown
| Question | Latency (ms) | Response Quality |
|----------|--------------|------------------|
| "hi" | 14,746 | Good |
| "hello" | 8,694 | Good |
| "How are you?" | 8,996 | Good |
| "What's your name?" | 25,898 | High (detailed personality) |
| "Explain quantum computing" | 11,540 | Good |
| "How does machine learning work?" | 28,520 | High (detailed explanation) |

#### LM Studio Log Analysis
- **Speculative Decoding**: Mixed model approach (7B main + 1B draft)
- **Draft Token Acceptance**: 74/130 (57%), 12/18 (67%), 84/127 (66%)
- **Finish Reasons**: Mix of "length" and "stop"
- **Response Characteristics**: Verbose, personality-focused, detailed explanations

---

### Model 2: Qwen2.5 Coder 7B Instruct
- **Model**: `qwen2.5-coder-7b-instruct@q4_k_m`
- **Draft Model**: `qwen2.5-coder-7b-instruct@q2_k`
- **Quantization**: Main q4_k_m, Draft q2_k (self-speculative decoding)
- **Test Date**: 2025-10-03 19:02:46

#### Performance Metrics
- **Average Latency**: 12,979ms (**20.9% FASTER**)
- **Total Time**: 77.9s (**20.8% FASTER**)
- **Success Rate**: 6/6 (100%)
- **Draft Efficiency**: 60-80% (self-speculative decoding)

#### Detailed Latency Breakdown
| Question | Latency (ms) | Speedup | Response Quality |
|----------|--------------|---------|------------------|
| "hi" | 13,886 | 5.8% faster | Good |
| "hello" | 8,513 | 2.1% faster | Good |
| "How are you?" | 8,858 | 1.5% faster | Good |
| "What's your name?" | 12,935 | **50.0% FASTER** | High (concise) |
| "Explain quantum computing" | 16,497 | 43.0% slower | High (technical) |
| "How does machine learning work?" | 17,187 | **39.7% FASTER** | High (technical) |

#### LM Studio Log Analysis
- **Speculative Decoding**: Self-speculative (same model, different quantization)
- **Draft Token Acceptance**: 12/15 (80%), 26/34 (76%), 20/33 (61%)
- **Finish Reasons**: Primarily "stop" (natural completion)
- **Response Characteristics**: Concise, technically focused, efficient

## Performance Comparison Summary

| Metric | Llama 3.2 PKD | Qwen2.5 Coder | Winner |
|--------|---------------|---------------|---------|
| **Average Latency** | 16,399ms | 12,979ms | üèÜ **Qwen2.5** |
| **Total Time** | 98.4s | 77.9s | üèÜ **Qwen2.5** |
| **Draft Efficiency** | 57-67% | 60-80% | üèÜ **Qwen2.5** |
| **Technical Questions** | Good | Excellent | üèÜ **Qwen2.5** |
| **Personality Responses** | Excellent | Good | üèÜ **Llama 3.2** |
| **Consistency** | Good | Excellent | üèÜ **Qwen2.5** |

## Key Insights

### Qwen2.5 Coder 7B Advantages
1. **Self-Speculative Decoding**: Same model architecture for main + draft = better alignment
2. **Higher Draft Efficiency**: 60-80% vs 57-67% acceptance rate
3. **Technical Excellence**: Superior performance on coding/technical questions
4. **Consistency**: More predictable latency across question types
5. **Speed**: 20.9% faster overall performance

### Llama 3.2 PKD Advantages
1. **Personality**: More detailed, personality-rich responses
2. **Verbosity**: More comprehensive explanations
3. **Creative Responses**: Better at generating engaging, detailed content

## Recommendations

### For Technical/Coding Tasks
**Recommended**: Qwen2.5 Coder 7B
- Faster performance
- Better technical accuracy
- More efficient speculative decoding
- Consistent latency

### For Conversational/Personality Tasks
**Recommended**: Llama 3.2 PKD
- Richer personality responses
- More detailed explanations
- Better creative output
- Engaging conversational style

## Technical Notes

### Speculative Decoding Comparison
- **Llama 3.2**: Mixed model approach (7B + 1B different architectures)
- **Qwen2.5**: Self-speculative (same model, q4_k_m + q2_k quantizations)

### Model Architecture
- **Llama 3.2**: General-purpose model optimized for personality
- **Qwen2.5**: Code-specialized model with general capabilities

---

### Model 3: OpenHermes 2.5 Mistral 7B
- **Model**: `openhermes-2.5-mistral-7b`
- **Draft Model**: `openhermes-2.5-mistral-7b@q2_k`
- **Quantization**: Main model (unknown), Draft q2_k (self-speculative decoding)
- **Test Date**: 2025-10-03 19:26:41

#### Performance Metrics
- **Average Latency**: 14,773ms (**10.0% FASTER than Llama 3.2, 13.8% SLOWER than Qwen2.5**)
- **Total Time**: 88.7s (**9.9% FASTER than Llama 3.2, 13.8% SLOWER than Qwen2.5**)
- **Success Rate**: 6/6 (100%)
- **Draft Efficiency**: Unknown (self-speculative decoding)

#### Detailed Latency Breakdown
| Question | Latency (ms) | vs Llama 3.2 | vs Qwen2.5 | Response Quality |
|----------|--------------|--------------|------------|------------------|
| "hi" | 14,134 | 4.1% faster | 1.8% faster | Good |
| "hello" | 8,962 | 3.0% faster | 5.3% slower | Good |
| "How are you?" | 9,735 | 8.2% slower | 9.9% slower | Good |
| "What's your name?" | 18,266 | **29.5% FASTER** | 41.2% slower | High (concise) |
| "Explain quantum computing" | 16,315 | 41.4% slower | 1.1% slower | High (technical) |
| "How does machine learning work?" | 21,230 | **25.6% FASTER** | 23.5% slower | High (detailed) |

#### LM Studio Log Analysis
- **Speculative Decoding**: Self-speculative (same model, different quantization)
- **Draft Token Acceptance**: Unknown (logs truncated)
- **Finish Reasons**: Mix of "stop" and other reasons
- **Response Characteristics**: Balanced, good personality, solid technical explanations

## Updated Performance Comparison Summary

| Metric | Llama 3.2 PKD | Qwen2.5 Coder | OpenHermes 2.5 | Winner |
|--------|---------------|---------------|----------------|---------|
| **Average Latency** | 16,399ms | 12,979ms | 14,773ms | üèÜ **Qwen2.5** |
| **Total Time** | 98.4s | 77.9s | 88.7s | üèÜ **Qwen2.5** |
| **Draft Efficiency** | 57-67% | 60-80% | Unknown | üèÜ **Qwen2.5** |
| **Technical Questions** | Good | Excellent | Good | üèÜ **Qwen2.5** |
| **Personality Responses** | Excellent | Good | Good | üèÜ **Llama 3.2** |
| **Consistency** | Good | Excellent | Good | üèÜ **Qwen2.5** |
| **Overall Balance** | Good | Excellent | Good | üèÜ **Qwen2.5** |

## Updated Key Insights

### OpenHermes 2.5 Mistral 7B Characteristics
1. **Balanced Performance**: Middle ground between Llama 3.2 and Qwen2.5
2. **Good Personality**: Solid personality responses, not as detailed as Llama 3.2
3. **Technical Competence**: Good technical explanations, not as specialized as Qwen2.5
4. **Consistent Speed**: More consistent than Llama 3.2, slower than Qwen2.5
5. **Self-Speculative**: Uses same model architecture for main + draft

### Updated Recommendations

### For Technical/Coding Tasks
**Recommended**: Qwen2.5 Coder 7B
- Fastest performance (13.0s avg)
- Best technical accuracy
- Most efficient speculative decoding
- Most consistent latency

### For Conversational/Personality Tasks
**Recommended**: Llama 3.2 PKD
- Richest personality responses
- Most detailed explanations
- Best creative output
- Most engaging conversational style

### For Balanced Use Cases
**Recommended**: OpenHermes 2.5 Mistral 7B
- Good balance of speed and quality
- Solid personality and technical capabilities
- Consistent performance
- Good middle-ground option

## Next Steps
- Test additional models for comparison
- Evaluate models on domain-specific tasks
- Optimize speculative decoding parameters
- Document model switching procedures

---
*Last Updated: 2025-10-03*
*Benchmark System: AIOS Clean v1.0*
