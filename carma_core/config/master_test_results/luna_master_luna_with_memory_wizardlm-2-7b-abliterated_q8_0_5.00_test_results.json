[
  {
    "model_name": "wizardlm-2-7b-abliterated@q8_0",
    "test_mode": "luna_with_memory",
    "test_start_time": "2025-09-21T17:14:52.946206",
    "sample_size": 1,
    "luna_config": {
      "luna_enabled": true,
      "session_memory": true,
      "rag_type": "luna"
    },
    "luna_personality": {
      "directness": 0.48507632675730583,
      "humor": 0.5128955605801462,
      "technical_depth": 0.48750760279048966,
      "emotional_engagement": 0.5346327097518184,
      "sexual_awareness": 0.5036164078236428,
      "skepticism": 0.42042667915770243,
      "enthusiasm": 0.28509543351532945,
      "authenticity": 0.5074096253293636
    },
    "questions": [
      {
        "id": 24,
        "question": "I am someone who seeks adventure",
        "trait": "openness",
        "reverse": false
      }
    ],
    "responses": {
      "q1": {
        "question": "I am someone who seeks adventure",
        "trait": "openness",
        "reverse": false,
        "response": "Hey there, fellow thrill-seeker! Adventure is definitely in my algorithmic DNA. What kind of adrenaline rush are you craving? Are we talking about something wild like bungee jumping or are we diving into the mysteries of a new city at night? Let's chat and see how deep your adventure bug bites.",
        "response_time": 12.785498,
        "lm_studio_metadata": {
          "completion_id": "chatcmpl-k91lyau44wl6gstamger",
          "model": "wizardlm-2-7b-abliterated@q8_0",
          "usage": {
            "prompt_tokens": 417,
            "completion_tokens": 75,
            "total_tokens": 492
          },
          "system_fingerprint": "wizardlm-2-7b-abliterated@q8_0",
          "finish_reason": "stop",
          "prompt_tokens": 417,
          "completion_tokens": 75,
          "total_tokens": 492
        }
      }
    },
    "quantitative_scores": {
      "q1": {
        "big_five_score": 5.0,
        "raw_agreement": 5.0,
        "authenticity": 0.3,
        "corporate_penalty": 0.0,
        "trait_expression": 0.0,
        "reverse_scored": false,
        "response_length": 295,
        "word_count": 51
      }
    },
    "big_five_profile": {
      "openness": 3.0,
      "conscientiousness": 3.0,
      "extraversion": 3.0,
      "agreeableness": 3.0,
      "neuroticism": 3.0
    },
    "performance_metrics": {
      "avg_response_time": 12.8,
      "avg_big_five_score": 5.0,
      "successful_responses": 1,
      "total_questions": 1,
      "success_rate": 100.0
    },
    "test_end_time": "2025-09-21T17:15:09.769706",
    "total_test_time_seconds": 16.8235,
    "analysis_metadata": {
      "total_data_points": 1,
      "parameters_hash": -4536621848584397687,
      "config_hash": 9061809237402428435,
      "framework_version": "2.0.0-master",
      "analysis_timestamp": "2025-09-21T17:15:09.770207",
      "file_size_estimate": 2788,
      "question_distribution": {
        "openness": 1
      },
      "response_length_stats": {
        "char_lengths": {
          "min": 295,
          "max": 295,
          "avg": 295.0
        },
        "word_counts": {
          "min": 51,
          "max": 51,
          "avg": 51.0
        }
      },
      "timing_analysis": {
        "response_times": {
          "min": 12.785498,
          "max": 12.785498,
          "avg": 12.785498,
          "variance": 0.0,
          "consistency_score": 1.0
        }
      },
      "score_distribution": {
        "overall": {
          "min": 5.0,
          "max": 5.0,
          "avg": 5.0,
          "variance": 0.0,
          "score_range": 0.0
        }
      },
      "parameter_fingerprint": "T0.7_K600_P0.9"
    },
    "id": "65bccf68-ed58-4b65-a1c9-a1938894bad6",
    "timestamp": "2025-09-24T14:54:06.377965Z"
  }
]