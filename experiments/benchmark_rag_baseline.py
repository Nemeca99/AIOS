#!/usr/bin/env python3
"""
Baseline RAG Latency Benchmark
Measures response times for baseline RAG system (without CARMA).
"""

import sys
import time
import argparse
import statistics
from pathlib import Path
from typing import List, Dict, Any
import json
import csv

def run_baseline_rag_query(query: str) -> Dict[str, Any]:
    """Run a single query through baseline RAG system."""
    start_time = time.time()
    
    try:
        # Simulate baseline RAG process:
        # 1. Generate embedding for query
        # 2. Search entire knowledge base
        # 3. Retrieve top-k documents
        # 4. Generate response
        
        # Simulate embedding generation (slower than CARMA)
        time.sleep(0.1)  # Simulate embedding API call
        
        # Simulate knowledge base search (much slower than CARMA)
        time.sleep(0.3)  # Simulate vector database search
        
        # Simulate document retrieval and processing
        time.sleep(0.2)  # Simulate document processing
        
        # Simulate response generation
        time.sleep(0.1)  # Simulate LLM generation
        
        response = f"Baseline RAG response to: {query}"
        
        end_time = time.time()
        
        return {
            'query': query,
            'response': response,
            'latency': end_time - start_time,
            'documents_searched': 1000,  # Simulate large knowledge base
            'success': True
        }
        
    except Exception as e:
        end_time = time.time()
        return {
            'query': query,
            'response': f"Error: {e}",
            'latency': end_time - start_time,
            'documents_searched': 0,
            'success': False,
            'error': str(e)
        }

def benchmark_baseline_rag_latency(questions: int = 50, iterations: int = 3) -> Dict[str, Any]:
    """Run baseline RAG latency benchmark."""
    
    print("üî• Baseline RAG Latency Benchmark")
    print("=" * 50)
    print(f"Questions: {questions}, Iterations: {iterations}")
    print()
    
    # Test queries
    test_queries = [
        "What is the capital of France?",
        "Explain quantum entanglement simply.",
        "Tell me a short story about a robot.",
        "How does machine learning work?",
        "What are the benefits of renewable energy?",
        "Describe the process of photosynthesis.",
        "What is artificial intelligence?",
        "How do computers process information?",
        "What is the meaning of life?",
        "Explain the theory of relativity.",
        "What are the principles of democracy?",
        "How does the human brain work?",
        "What is climate change?",
        "Describe the water cycle.",
        "What is the history of the internet?",
        "How do vaccines work?",
        "What is sustainable development?",
        "Explain the concept of evolution.",
        "What are the benefits of exercise?",
        "How does music affect the brain?"
    ]
    
    # Extend queries if needed
    while len(test_queries) < questions:
        test_queries.extend(test_queries[:min(20, questions - len(test_queries))])
    
    test_queries = test_queries[:questions]
    
    # Run benchmark iterations
    all_results = []
    
    for iteration in range(iterations):
        print(f"Iteration {iteration + 1}: Running {questions} queries...")
        
        iteration_results = []
        iteration_times = []
        
        for i, query in enumerate(test_queries):
            result = run_baseline_rag_query(query)
            iteration_results.append(result)
            iteration_times.append(result['latency'])
            
            if (i + 1) % 10 == 0:
                print(f"  Completed {i + 1}/{questions} queries...")
        
        avg_time = statistics.mean(iteration_times)
        min_time = min(iteration_times)
        max_time = max(iteration_times)
        std_time = statistics.stdev(iteration_times) if len(iteration_times) > 1 else 0
        
        print(f"Iteration {iteration + 1}: {avg_time:.1f}s average")
        
        all_results.extend(iteration_results)
    
    # Calculate overall statistics
    all_times = [r['latency'] for r in all_results if r['success']]
    successful_queries = len([r for r in all_results if r['success']])
    failed_queries = len(all_results) - successful_queries
    
    if all_times:
        avg_latency = statistics.mean(all_times)
        min_latency = min(all_times)
        max_latency = max(all_times)
        std_latency = statistics.stdev(all_times) if len(all_times) > 1 else 0
        median_latency = statistics.median(all_times)
    else:
        avg_latency = min_latency = max_latency = std_latency = median_latency = 0
    
    # Print results
    print()
    print("üìä Results:")
    print(f"   Average latency: {avg_latency:.2f}s")
    print(f"   Min latency: {min_latency:.2f}s")
    print(f"   Max latency: {max_latency:.2f}s")
    print(f"   Median latency: {median_latency:.2f}s")
    print(f"   Standard deviation: {std_latency:.2f}s")
    print(f"   Successful queries: {successful_queries}/{len(all_results)}")
    print(f"   Failed queries: {failed_queries}")
    
    # Save results
    results_data = {
        'timestamp': time.time(),
        'system': 'Baseline RAG',
        'questions': questions,
        'iterations': iterations,
        'total_queries': len(all_results),
        'successful_queries': successful_queries,
        'failed_queries': failed_queries,
        'avg_latency': avg_latency,
        'min_latency': min_latency,
        'max_latency': max_latency,
        'median_latency': median_latency,
        'std_latency': std_latency,
        'results': all_results
    }
    
    # Save to JSON
    os.makedirs('reports', exist_ok=True)
    json_file = f'reports/baseline_rag_latency_benchmark_{int(time.time())}.json'
    with open(json_file, 'w') as f:
        json.dump(results_data, f, indent=2)
    
    # Save to CSV
    csv_file = f'reports/baseline_rag_latency_benchmark_{int(time.time())}.csv'
    with open(csv_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['query', 'latency', 'success', 'documents_searched', 'error'])
        for result in all_results:
            writer.writerow([
                result['query'],
                result['latency'],
                result['success'],
                result['documents_searched'],
                result.get('error', '')
            ])
    
    print(f"üìÅ Results saved to: {json_file}")
    print(f"üìÅ CSV saved to: {csv_file}")
    
    return results_data

def main():
    parser = argparse.ArgumentParser(description="Benchmark baseline RAG system latency")
    parser.add_argument("--questions", type=int, default=50, help="Number of questions to test")
    parser.add_argument("--iterations", type=int, default=3, help="Number of iterations to run")
    
    args = parser.parse_args()
    
    results = benchmark_baseline_rag_latency(args.questions, args.iterations)
    
    if 'error' in results:
        print(f"\n‚ùå Benchmark failed: {results['error']}")
        sys.exit(1)
    else:
        print(f"\n‚úÖ Benchmark completed successfully!")
        print(f"üêå Baseline RAG average latency: {results['avg_latency']:.2f}s")

if __name__ == "__main__":
    import os
    main()
